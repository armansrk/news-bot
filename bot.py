import os
import re
import time
import requests
import feedparser
from bs4 import BeautifulSoup

# ================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ==================
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
SEEN_FILE = "seen.txt"

COINS = [
    "Ø¨ÛŒØª Ú©ÙˆÛŒÙ†", "Ø§ØªØ±ÛŒÙˆÙ…", "Ø¯ÙˆØ¬", "Ø±ÛŒÙ¾Ù„", "Ú©Ø§Ø±Ø¯Ø§Ù†Ùˆ", "Ø³ÙˆÙ„Ø§Ù†Ø§",
    "Ø´ÛŒØ¨Ø§", "Ù¾ÙˆÙ„Ú©Ø§Ø¯Ø§Øª", "Ø¨ÛŒØª Ú©ÙˆÛŒÙ† Ú©Ø´",
    "Bitcoin", "BTC", "Ethereum", "ETH", "XRP", "SOL", "DOGE", "ADA",
    "ETF", "SEC"
]

# RSS Ù…Ù†Ø§Ø¨Ø¹ (Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¨Ø¹Ø¯Ø§Ù‹ Ø¨ÛŒØ´ØªØ±Ø´ Ú©Ù†ÛŒ)
RSS_FEEDS = [
    "https://arzdigital.com/feed/",
]

HEADERS = {"User-Agent": "Mozilla/5.0 (NewsBot)"}


# ================== Ø®ÙˆØ§Ù†Ø¯Ù†/Ù†ÙˆØ´ØªÙ† seen.txt ==================
def load_seen() -> set:
    if not os.path.exists(SEEN_FILE):
        return set()
    with open(SEEN_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())


def save_seen(seen: set):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        for url in sorted(seen):
            f.write(url + "\n")


# ================== ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ ==================
def matches_keywords(title: str) -> bool:
    t = (title or "").lower()
    return any(k.lower() in t for k in COINS)


# ================== Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ ==================
def extract_summary_from_url(url: str, max_chars: int = 420) -> str:
    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.raise_for_status()

        soup = BeautifulSoup(r.text, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()

        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        text = " ".join([p for p in paragraphs if p and len(p) > 30])
        text = re.sub(r"\s+", " ", text).strip()

        if not text:
            return "Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

        summary = text[:max_chars]
        if len(text) > max_chars:
            summary += "â€¦"
        return summary

    except Exception:
        return "Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."


# ================== Ú¯Ø±ÙØªÙ† Ø®Ø¨Ø± Ø§Ø² RSS ==================
def get_news_from_rss():
    items = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:40]:
                title = getattr(entry, "title", "").strip()
                link = getattr(entry, "link", "").strip()

                if title and link and matches_keywords(title):
                    items.append({"title": title, "link": link})

        except Exception:
            continue
    return items


# ================== Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… (HTTP API) ==================
def send_telegram_message(html_text: str):
    api_url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHANNEL_ID,
        "text": html_text,
        "parse_mode": "HTML",
        "disable_web_page_preview": False
    }
    r = requests.post(api_url, json=payload, timeout=20)
    r.raise_for_status()


# ================== Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø¨Ø§Øª ==================
def job():
    if not BOT_TOKEN or not CHANNEL_ID:
        raise RuntimeError("BOT_TOKEN Ùˆ CHANNEL_ID Ø±Ø§ Ø¯Ø± GitHub Secrets Ø³Øª Ú©Ù†.")

    seen = load_seen()
    news = get_news_from_rss()

    sent = 0
    for item in news:
        url = item["link"]
        title = item["title"]

        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
        if url in seen:
            continue

        summary = extract_summary_from_url(url)

        message = (
            f"ğŸ”¹ <b>{title}</b>\n\n"
            f"{summary}\n\n"
            f"ğŸ”— <a href='{url}'>Ø§Ø¯Ø§Ù…Ù‡ Ø®Ø¨Ø±</a>"
        )

        send_telegram_message(message)
        seen.add(url)
        sent += 1
        time.sleep(1)  # Ø¶Ø¯ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªÙ„Ú¯Ø±Ø§Ù…

    save_seen(seen)
    print(f"âœ… {sent} Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† ØªÚ©Ø±Ø§Ø±)")


# âœ… Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…ÙˆÙ†ÛŒÙ‡ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ø´Ù‡
if name == "main":
    job()
